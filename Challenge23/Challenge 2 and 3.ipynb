{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Challenge 2\n",
    "\n",
    "In this Challenge, we will:\n",
    "\n",
    "- Create a training script using the code from Challenge 1\n",
    "- Perform unit tests on the training script\n",
    "- Run an experiment from a Python Script using the ScriptRunConfig() object\n",
    "- Register the model into Azure ML and log the relevant metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1\n",
    " Ensure that you move your training data from the data folder in Challenge 1 to the data folder found in the Challenge23 directory.\n",
    "\n",
    " In the below cell, we create a separate folder to contain our Experiment files, called the **driver-training** folder. This is done as delineation and keeps our process neat and organized. This will become especially helpful when we get to Challenge 3 and we create a separate folder containing files ready for deployed in the the **driver-service** folder. More on that later.\n",
    "\n",
    " In **driver-training**, we copy in our training data and have it as a reference for the next two Challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "training_folder = 'driver-training'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "# Copy the data file into the experiment folder\n",
    "shutil.copy('data/porto_seguro_safe_driver_prediction_input.csv', os.path.join(training_folder, \"porto_seguro_safe_driver_prediction_input.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the version of the LightGBM model. We do this to ensure that we have the latest model version. You should see version 2.3.0 being returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\"lightgbm=={}\".format(lgb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a distributed gradient boosting framework developed by Microsoft. It's based on [decision tree algorithms](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/two-class-boosted-decision-tree#bkmk_research) and is typically used for classification problems with small datasets (< 1 million examples). Gradient boosting is one of the most powerful techniques for building predictive models, because it has:\n",
    "\n",
    "- Fast training speed and higher efficiency\n",
    "- Lower memory usage\n",
    "- Better accuracy\n",
    "\n",
    "If we had to break the concept down further: LightGBM is based on decision tree algorithms. Think of these algorithms as a flowchart that can be drawn out to have the structure of a tree. \n",
    "\n",
    "What we mean by this is that the internal nodes of the trees represent a test or a question on an attribute; each branch is the possible outcome of the question asked, and the terminal node, which is also called as the leaf node, denotes a classification label (i.e. 0 or 1, cat or dog, claim or no claim).\n",
    "\n",
    "In a decision tree, we have several predictor variables. Depending upon these predictor variables, try to predict the so-called response variable. \n",
    "\n",
    "In our case, our predictor variables could be the driver's age, experience, vehicle type, health conditions, eyesight abilities, vehicle mileage, last date of vehicle service. Using these variables - which have been converted into numbers using feature engineering and data cleaning - we predict whether or not a driver will submit an insurance claim; this is our response variable.\n",
    "\n",
    "For further reading, have a look at this post written by [Microsoft Research](https://www.microsoft.com/en-us/research/project/lightgbm/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: Creating the training script: train.py \n",
    "\n",
    "In machine learning, we require a training script to train our machine learning model. For dev/test purposes, this can be done in Jupyter notebook cells like the ones you see below. However, if we think operationally, when we want to deploy a model at scale for use at a customer site, we have to transform the machine learning model training steps into functions as part of a Python Script. Doing so enables us to build a model in the context of software engineering principles, which then in turns enables us to build applications or a CICD pipeline for model deployment.\n",
    "\n",
    "This file defines the key functions required to train the model. \n",
    "The file can be invoked with `python train.py` for development purposes.\n",
    "\n",
    "See the comment in the cell below for the reason for the code.\n",
    "\n",
    "In this cell, refactor the code from the Challenge 1 notebook into a Python Script by completing the commented ## TODO sections of the train.py cell in the Challenge 2 notebook.\n",
    "\n",
    "Open up your Challenge 1 notebook to serve as reference of what code we ran in Challenge 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile $training_folder/train.py\n",
    "# The line above uses the %%writefile command to transfer everything we write in this cell into a Python script called train.py within your training_folder.\n",
    "# Import our libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm #Importing the lightgbm model package from Microsoft\n",
    "\n",
    "\n",
    "def split_data(data_df):\n",
    "    \"\"\"Split a dataframe into training and validation datasets\n",
    "       We train our model using the training dataset and then \n",
    "       tune the model using our validation dataset. Splitting our\n",
    "       data, especially for supervised learning - like classification -\n",
    "       ensures that we don't create a biased model and gives us a way\n",
    "       to evaluate model performance on data the model hasn't \"seen\"\n",
    "       or learnt from. \n",
    "    \"\"\"\n",
    "    \n",
    "    ## TODO\n",
    "    \n",
    "    return (train_data, valid_data)\n",
    "\n",
    "\n",
    "def train_model(data, parameters):\n",
    "    \"\"\"Train a model with the given datasets and parameters\"\"\"\n",
    "    # The object returned by split_data is a tuple.\n",
    "    # Access train_data with data[0] and valid_data with data[1]\n",
    "    \n",
    "    ## TODO\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_metrics(model, data):\n",
    "    \"\"\"Construct a dictionary of metrics for the model\"\"\"\n",
    "    \n",
    "    ## TODO\n",
    "    \n",
    "    return model_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"This method invokes the training functions for development purposes\"\"\"\n",
    "    \n",
    "    # Read data from a file\n",
    "    data_df = pd.read_csv('porto_seguro_safe_driver_prediction_input.csv')\n",
    "\n",
    "    # Hard code the parameters for training the model\n",
    "    parameters = {\n",
    "        'learning_rate': 0.02,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'sub_feature': 0.7,\n",
    "        'num_leaves': 60,\n",
    "        'min_data': 100,\n",
    "        'min_hessian': 1,\n",
    "        'verbose': 2\n",
    "    }\n",
    "\n",
    "    # Call the functions defined in this file and assign them to variables to ensure the outputs of one function can be fed as inputs into the next function.\n",
    "    ## TODO\n",
    "    \n",
    "    # Print the resulting metrics for the model\n",
    "    ## TODO\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3\n",
    "\n",
    "Now we need to pass our Unit Tests. This ensures that our code makes sense. We pass through dummy data to our training script and expect to pass 3 tests. \n",
    "\n",
    "Ensure the provided unit tests pass for train.py by calling pytest in the Terminal, which is started from the Notebook's UI. Browse the contents of the file with tests to understand what types of tests may be relevant. Refer to [Step 3 in the HOL for Challenge 2](https://github.com/tgokal/EY-MSFTAI-Workshop3/blob/master/Challenge23/HOL_Challenge_2.MD#-steps-) for help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 4\n",
    "\n",
    "Make the `train.py` and `test_train.py` files pass linting with `flake8` in the Terminal. You do not need to fix every single linting error here. This is for your understanding of how to produce quality code and does not impact the output and outcome. Once you have a good understnading of what the errors mean and how you would resolve them, complete and run the remaining cells of the Challenge 2 notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 5: Create a JSON file for the model parameters: parameters.json\n",
    "\n",
    "This file will specify the parameters used to train the model.\n",
    "\n",
    "We put this information into a separate file because it gives us independence and a central truth in how we play around with our variables. \n",
    "\n",
    "Rather than changing every occurence of a variable in a training script, we can play around with these constants from a single file.\n",
    "\n",
    "In particular, our parameters include:\n",
    "\n",
    "- `Learning rate`: Controls the rate at which an algorithm learns the values of the parameters.\n",
    "\n",
    "- `Boosting type`: Type of algorithm used in technique of gradient boosting. Here, we use 'gbdt' = gradient boosting decision tree.\n",
    "\n",
    "- `Objective`: Algorithm goal. Here, we're doing binary classification. \n",
    "\n",
    "- `Metric`: A score used to assess performance of our algorithm. Here, we use AUC = area under the curve. The closer to 1.00, the \"better\" the model performance.\n",
    "\n",
    "- `Sub feature`: Another term for feature fraction. Selects a subset of features on each tree. Here, we set it to be 0.7 = LightGBM will select 70% of features before training each tree.\n",
    "\n",
    "- `Number of leaves in the decision tree`: Controls the complexity of the model. More leaves increases the accuracy on the training set, but also the chance of overfitting the model to the data.\n",
    "\n",
    "- `Min data`: Minimal number of data in 1 leaf. We use this parameter to control over-fitting. Default is typically 20 data samples per leaf.\n",
    "\n",
    "- `Min Hessian`: Minimal sum of a value called the Hessian. Used to account for the number of observations in a lead of our split. If the value > 1 the decision tree that we create won't be that large and will be a lower complexity model.\n",
    "\n",
    "- `Verbose`: Useful for debugging, sets the level of information returned back during model training. < 0 = Fatal only, 0 = Error only, 1 = Warnings only, > 1 = Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/parameters.json\n",
    "{\n",
    "    \"training\":\n",
    "    {\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"sub_feature\": 0.7,\n",
    "        \"num_leaves\": 60,\n",
    "        \"min_data\": 100,\n",
    "        \"min_hessian\": 1,\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 6: Creating a \"remote control\" training script: driver_training.py\n",
    "This file will be the entry script when running an Azure ML context - it calls the training script but is shaping it in the context of the Azure ML platform.\n",
    "It calls the functions defined in train.py for data preparation and training, but reads parameters from a file, and logs output to the Azure ML context.  \n",
    "\n",
    "The file can be invoked with `python driver_training.py` for development purposes.\n",
    "\n",
    "Complete the `##TODO` sections of the `driver-training.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/driver_training.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "from azureml.core import Run\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Import functions from train.py\n",
    "from train import split_data, train_model, get_model_metrics\n",
    "\n",
    "# Get the output folder for the model from the '--output_folder' parameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--output_folder', type=str, dest='output_folder', default=\"outputs\")\n",
    "args = parser.parse_args()\n",
    "output_folder = args.output_folder\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the safe driver prediction dataset\n",
    "train_df = pd.read_csv('porto_seguro_safe_driver_prediction_input.csv')\n",
    "\n",
    "# Load the parameters for training the model from the file\n",
    "with open(\"parameters.json\") as f:\n",
    "    pars = json.load(f)\n",
    "    parameters = pars[\"training\"]\n",
    "\n",
    "# Log each of the parameters to the run\n",
    "for param_name, param_value in parameters.items():\n",
    "    run.log(param_name, param_value)\n",
    "    \n",
    "# Use the functions imported from train.py to prepare data, train the model, and calculate the metrics\n",
    "## TODO\n",
    "\n",
    "# Log the metrics variable using run.log()\n",
    "\n",
    "# Save the trained model to the output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = output_folder + \"/porto_seguro_safe_driver_model.pkl\"\n",
    "joblib.dump(value=model, filename=output_path)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining an outputs folder for the model artefact\n",
    "\n",
    "output_folder = \"outputs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = output_folder + \"/porto_seguro_safe_driver_model.pkl\"\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 7: Use ScriptRunConfig to run the script as an Experiment\n",
    "\n",
    "Now that we've defined our model parameters, training script and then remote training script with the Azure ML context, we can submit it for training to Azure ML as an experiment. However, since we've coded up our own model, we submit this to the platform using the ScriptRunConfig object. A ScriptRunConfig packages together the configuration information needed to submit a run in Azure ML, including the script, compute target, environment, and any distributed job-specific configs.\n",
    "\n",
    "A ScriptRunConfig object is used to configure the information necessary for submitting a training run as part of an Experiment. When a run is submitted using a ScriptRunConfig object, the submit method returns an object of type ScriptRun. Then returned ScriptRun object gives you programmatic access to information about the training run. ScriptRun is a child class of Run.\n",
    "\n",
    "The key concept to remember is that there are different configuration objects that are used to submit an experiment, based on what kind of run you want to trigger. The type of the configuration object then informs what child class of Run you get back from the submit method. When you pass a ScriptRunConfig object in a call to Experiment's submit method, you get back a ScriptRun object. \n",
    "\n",
    "Complete each of the steps in separate Notebook cells.\n",
    "\n",
    "1. First, create a [compute cluster](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python). This compute cluster will be used to train our ML model and we can programmatically create it using a code sample like the one provided in the link. \n",
    "\n",
    "    Name your compute cluser as `drivercluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an compute cluster\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Create an environment](https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment/#in-azure-ml-sdk) and add the conda and pip packages beforing registering it into the Workspace. Use the pip, joblib, scikit-learn, pandas and lightgbm as the only pip packages. \n",
    "\n",
    "    **Don't just copy and paste the code from the link - think about why we need/don't need PyTorch as a channel.**\n",
    "    \n",
    "     Rename your environment accordingly to `driver_training_env`.\n",
    "\n",
    "    This makes our packages as part of a reuseable software environment. When we train a model in Azure ML, it pulls a Docker image for training, referencing the YML file which contains these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "\n",
    "# Include this line \n",
    "conda.add_conda_package('python=3.7')\n",
    "\n",
    "## TODO\n",
    "\n",
    "# This will be the last line of code\n",
    "env = Environment('driver_training_env')\n",
    "env.python.conda_dependencies = conda\n",
    "env.register(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Create an experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py), retrieve the compute target and environment before configuring and submitting your training run using the ScriptRunConfig() object on your local machine. \n",
    "\n",
    "    This step defines Experiment in Azure ML and retrieves the compute target, environment and submits the training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment\n",
    "## TODO\n",
    "\n",
    "# Include this variable\n",
    "args = \"outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop\n",
    "\n",
    "Switch back to your Azure ML UI and observe the training of the model in the Experiments tab. This training should take 5 mins overall and you should see a Completed green tick under the Details Tap of your Experiment Run. \n",
    "\n",
    "**Do not run the below cells before you have a successfully completed trained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 8\n",
    "\n",
    "View the learning_rate parameter in the configuration file, execute a run against Azure ML compute to train the model and verify that the metrics are logged. Change the learning_rate value and then execute another run and see the values changed in the Azure ML service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting metrics\n",
    "metrics = run.get_metrics()\n",
    "for k, v in metrics.items():\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 9\n",
    "\n",
    "Register the model in the Azure ML model repository using the provided notebook code in your Azure ML workspace - using tags to record the AUC metric in the registration so that the quality of the model is registered and not just the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "run.register_model(model_path='outputs/porto_seguro_safe_driver_model.pkl', model_name='porto_seguro_safe_driver_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop\n",
    "\n",
    "Switch back to your Azure ML UI and observe the model artefact in the Models tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coaching Questions\n",
    "\n",
    "Once you've completed the Challenge, tag your Coach in your Team Channel and discuss the following questions with them:\n",
    "\n",
    "- What is the benefit of separating the training code out of the notebook?\n",
    "\n",
    "- What is the benefit of running your experiments using Azure Machine Learning?\n",
    "\n",
    "- What are the benefits of using a model repository and tracking the metrics and other parameters?\n",
    "\n",
    "- What is your experience in your organization/customers regarding doing things like model registration and tracking training metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "To have completed this Challenge, you should have:\n",
    "\n",
    "- Have a fully functional Challenge 2 notebook (.ipynb)\n",
    "\n",
    "- Successfully runs your experiment on Azure ML and can see the logged AUC metrics and trained model in the run results using two different parameter configuration values.\n",
    "\n",
    "- Successfully registers the trained model and tags the model with the AUC metric.\n",
    "\n",
    "- Demonstrate that the unit tests passs against the Python training code using pytest.\n",
    "\n",
    "- Use flake8 to demonstrate that train.py and test_train.py conform to the PEP 8 style guide for Python code.\n",
    "\n",
    "- Do a local run of the notebook and show the metrics associated both with the run and the registered model in Azure ML.\n",
    "\n",
    "- Discuss the Coaching Questions above with your Coach and Team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Welcome to Challenge 3 </u>\n",
    "\n",
    "You just transformed the experimental notebook into a Python script that can be managed and run independently of the notebook environment. You used the script to train the model, then you used code in the notebook to register the model that the script produced.\n",
    "\n",
    "To keep the training, registration and future steps like evaluation as easily reproducible as possible, we can encapsulate the model training and registration steps in an Azure ML pipeline which utilizes provisioned on-demand scalable Azure compute targets. The Azure compute target optimizes the time spent running training with a full set of data and can scale down to no cost when the job is done.\n",
    "\n",
    "In order to improve the insurance application, we can use a real-time prediction of the likelihood that a driver will file a claim. To accomplish this objective, we'll be deploying the registered model as a real-time inferencing REST service using the provisioned model scoring script.\n",
    "\n",
    "In this Challenge, we will:\n",
    "- Retrieve the most recent version of the registered insurance claim prediction model.\n",
    "\n",
    "- Create a socring script which includes an `init` function that loads the registered model and a `run` function that uses it to predict claim classifications for new driver data.\n",
    "\n",
    "- Define the container environment that includes the Python packages required by your scoring script.\n",
    "\n",
    "- Deploy the model as an Azure Container Instance service.\n",
    "\n",
    "- Testing the deployed service by submitting a REST request to its endpoint and review the predictions it returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1\n",
    "\n",
    "In the below cell, we'll retrieve the most recent version of the registered insurance claim prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine model name and version\n",
    "model = ws.models['porto_seguro_safe_driver_model.pkl']\n",
    "print(model.name, 'version', model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're creating a folder for the web service files. This provides delineation of training and deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_name = 'driver-service'\n",
    "\n",
    "# Create a folder for the web service files\n",
    "experiment_folder = './' + folder_name\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(folder_name, 'folder created.')\n",
    "\n",
    "# Set path for scoring script\n",
    "script_file = os.path.join(experiment_folder,\"score_driver.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: Define the scoring script\n",
    "\n",
    "Our scoring script is the next step. This script contains step by step instructions of what will happen when we deploy to our inference compute, in our case this will be an Azure Container Instance. What happens is this - Azure Machine Learning will create a docker image for you automatically that will package up the model, the runtime and the scoring script. This docker image will then be sent and built on the Azure Container Instance. \n",
    "\n",
    "Why does this matter? Often data scientists - especially at our customers - aren't familiar with the software engineer lifecycle and using technologies like Docker to deliver software as containers. Traditionally, data scientists have been doing statistical experiments - like the one we've just done - in a Jupyter Notebook, like this one. And that's it. No deployment. \n",
    "\n",
    "But of course, that doesn't make sense for the real-world as we put models to action. Azure Machine Learning makes it easier for data scientists to transition and learn software engineering techniques. Another element of this which you'll see later on; automatically generating a REST API endpoint for your model by using the InferenceConfig and returning a service scoring URI. \n",
    "\n",
    "Coming back to the below cell. \n",
    "\n",
    "The `init` method loads and de-serialises the model from the .pkl file and received parameters packed into the model artefact, like the ones we defined above.\n",
    "\n",
    "The `run` method passes the parameters and data to the model for scoring and then returns the resulting predictions and probabilities.\n",
    "\n",
    "Complete the below # TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_file\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the deployed model file and load it\n",
    "    model_path = Model.get_model_path('porto_seguro_safe_driver_model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # Get the corresponding classname for each prediction (0 or 1)\n",
    "    classnames = ['no claim', 'claim']\n",
    "    predicted_classes = []\n",
    "    probabilities = [prediction for prediction in predictions]\n",
    "\n",
    "    # TODO: \n",
    "    \"\"\"Write a for-loop which appends classnames[0] to the predicted_classes\n",
    "       list if the probability < 0.5, else to classnames[1]\n",
    "    \"\"\"\n",
    "    # Return the predictions\n",
    "    return (probabilities, predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, you should have defined the `run` function for the scoring script which will call the model predictions and returns the probabilities and predicted classes. This is building logic so that we're interpreting the probability to what it means for the business outcome - taking the spectrum of probabilities that exist between 0 and 1 and sorting it into a binary class; the driver will make a claim vs. the driver won't make a claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Define the container environment\n",
    "\n",
    "Here, we're creating a YAML file which contains our dependencies for the model to run. \n",
    "\n",
    "Why does this matter? \n",
    "\n",
    "When we deploy our model to Azure Container Instances, Azure Machine Learning creates a scoring image that contains your model, packages and all needed files for the model to be built and successfully predit on new data on the Container Instance. Think of it as screenshot of what software is needed for the model to work.\n",
    "\n",
    "For this to work, the software environment which we used to train our model - all 3rd party libraries - need to be added so that when we deploy our model to Azure Container Instances, the software rebuilds in the exact same environment that was used for training and development. \n",
    "\n",
    "It sounds like a complex process, but all we do is specify our packages and create our YAML file which is passed into our scoring Docker image using the InferenceConfig later on. \n",
    "\n",
    "You just need to \"point\" to the packages that we used for model building and understand how we're generating the YAML file below using the CondaDependencies class.\n",
    "\n",
    "Use this [link](https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/environment#in-azure-ml-sdk) as reference to complete the below #TODO, where you're adding `scikit-learn`, `lightgbm` and `pandas` dependencies for the model.\n",
    "\n",
    "If you're familiar with creating your own Docker image, you could bring your own - creating your own Dockerfile and registering it as an environment in Azure Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "# TODO: Add the dependencies for our model (AzureML defaults is already included)\n",
    "\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = os.path.join(experiment_folder,\"driver_env.yml\")\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)\n",
    "\n",
    "# Print the .yml file\n",
    "with open(env_file,\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 4: Configure the InferenceConfig\n",
    "\n",
    "Here, we'll use this link to deploy the model to Azure Container Instances once we configure the InferenceConfig.\n",
    "\n",
    "As we deploy the model using the InferenceConfig object, Azure Machine Learning creates a docker image including the model and the scoring script and then pushes this to Azure Container Registry. This image is like a screenshot of your model, dependencies, scoring script and all the necessary code for it to work on a compute instance. We take this image and deploy it as a container on Azure Container Instances.\n",
    "\n",
    "Use this [link](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py#remarks) to get started on what you need to do to complete the # TODO below. \n",
    "\n",
    "**Note: the example used in the above link has a parameter mistake. Instead of the parameter `environment`, use `conda_file`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "# TODO: Configure the scoring environment\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.state)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for webservice_name in ws.webservices:\n",
    "    print(webservice_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 5: Use Webservice for 1 driver\n",
    "\n",
    "Now that we've successfully created a service scoring URI, we need to test it out. \n",
    "\n",
    "In the below cell, we're submitting data - where each entry represents a feature that has been transformed into a number using a technique like one-hot encoding or generating a probability distribution - to the service endpoint as a JSON document.\n",
    "\n",
    "Once the call to the web service is successfully, we access the predictions by indexing the returning array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "x_new = [[0,1,8,1,0,0,1,0,0,0,0,0,0,0,12,1,0,0,0.5,0.3,0.610327781,7,1,-1,0,-1,1,1,1,2,1,65,1,0.316227766,0.669556409,0.352136337,3.464101615,0.1,0.8,0.6,1,1,6,3,6,2,9,1,1,1,12,0,1,1,0,0,1]]\n",
    "print ('Patient: {}'.format(x_new[0]))\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data (the web service will also accept the data in binary format)\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "print(\"Probability of claim is:\", predictions[0][0])\n",
    "print(\"Predicted class is: \", predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Webservice for multiple drivers\n",
    "\n",
    "So, we've down it for 1 driver, now we're testing if our endpoint works for multiple drivers. In this case, we're testing for two drivers. Out of interest of time, we're not testing further, but you should always scale out and test how the model received data for multiple inputs. Typically, as it works for a number > 1, it should in theory work for all.\n",
    "\n",
    "Same process as the above, but now we're submitting 2 feature arrays, each representing a different driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# This time our input is an array of two feature arrays\n",
    "x_new = [[0,1,8,1,0,0,1,0,0,0,0,0,0,0,12,1,0,0,0.5,0.3,0.610327781,7,1,-1,0,-1,1,1,1,2,1,65,1,0.316227766,0.669556409,0.352136337,3.464101615,0.1,0.8,0.6,1,1,6,3,6,2,9,1,1,1,12,0,1,1,0,0,1],\n",
    "         [4,2,5,1,0,0,0,0,1,0,0,0,0,0,5,1,0,0,0.9,0.5,0.771362431,4,1,-1,0,0,11,1,1,0,1,103,1,0.316227766,0.60632002,0.358329457,2.828427125,0.4,0.5,0.4,3,3,8,4,10,2,7,2,0,3,10,0,0,1,1,0,1]]\n",
    "# Convert the array or arrays to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "# Get the predicted classes.\n",
    "for i in range(len(predictions)):\n",
    "    print(\"Driver {} submits {} with a probability of {}\".format(i+1, predictions[1][i], round(predictions[0][i], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return the scoring endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 6: Submit POST Request to Webservice\n",
    "\n",
    "Now that we've gained confidence that the endpoint can work for multiple drivers, we're going to use the endpoint but access it using a POST request, which is what developers in the Customer will do once deployed. \n",
    "\n",
    "If you're new to sending POST requests, read up about it [here](https://docs.microsoft.com/en-us/learn/modules/use-apis-discover-museum-art/2-what-is-api) and [here](https://www.w3schools.com/tags/ref_httpmethods.asp). \n",
    "\n",
    "Same process as the above, but now we're submitting 2 feature arrays, each representing a different driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0,1,8,1,0,0,1,0,0,0,0,0,0,0,12,1,0,0,0.5,0.3,0.610327781,7,1,-1,0,-1,1,1,1,2,1,65,1,0.316227766,0.669556409,0.352136337,3.464101615,0.1,0.8,0.6,1,1,6,3,6,2,9,1,1,1,12,0,1,1,0,0,1],[4,2,5,1,0,0,0,0,1,0,0,0,0,0,5,1,0,0,0.9,0.5,0.771362431,4,1,-1,0,0,11,1,1,0,1,103,1,0.316227766,0.60632002,0.358329457,2.828427125,0.4,0.5,0.4,3,3,8,4,10,2,7,2,0,3,10,0,0,1,1,0,1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "predictions = (requests.post(endpoint, input_json, headers = headers)).json()\n",
    "\n",
    "# Get the predicted classes.\n",
    "for i in range(len(predictions)):\n",
    "    print(\"Driver {} submits {} with a probability of {}\".format(i+1, predictions[1][i], round(predictions[0][i], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coaching Questions\n",
    "\n",
    "- What are the benefits of splitting the ML process into steps?\n",
    "\n",
    "- What are the benefits of using the InferenceConfig object for deployment?\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- Define the `run` function for the scoring script which will call the model predictions and returns the probabilities and predicted classes.\n",
    "\n",
    "- Add the model dependencies into the container environment.\n",
    "\n",
    "- Define the InferenceConfig() which is used to deploy the model to Azure Container Instances.\n",
    "\n",
    "- Successfully deploy the trained model as a REST service in Azure Container Instance and test its endpoint.\n",
    "\n",
    "- Discuss the Coaching Questions with your Coach and Team.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
